---
title: "Tarea 1"
output:
  html_document:
    df_print: paged
---

![](C:/Users\nicol\Escritorio\Pensamiento\Statistical_Thinking\T1\banner.png)

<center> <h1>Tarea 1: Foundations</h1> </center>
<center><strong>CC6104: Statistical Thinking</strong></center>
#### **Integrantes :** 

- Nicolás García Ríos

#### **Cuerpo Docente:**

- Profesor: Felipe Bravo M.
- Auxiliar: Sebastian Bustos e Ignacio Meza D.
            

#### **Fecha límite de entrega:**

### **Índice:**

1. [Objetivo](#id1)
2. [Instrucciones](#id2)
3. [Referencias](#id3)
2. [Primera Parte: Preguntas Teóricas](#id4)
3. [Segunda Parte: Elaboración de Código](#id5)

### **Objetivo**<a name="id1"></a>

Bienvenid@s a la primera tarea del curso Statistical Thinking. Esta tarea tiene como objetivo evaluar los contenidos teóricos de la primera parte del curso, los cuales se enfocan principalmente en análisis exploratorio de datos y conceptos introductorios de probabilidades. Si aún no han visto las clases, se recomienda visitar los enlaces de las referencias.

La tarea consta de una parte teórica que busca evaluar conceptos vistos en clases. Seguido por una parte práctica con el fín de introducirlos a la programación en R enfocada en el análisis estadístico de datos. 

### **Instrucciones:**<a name="id2"></a>

- La tarea se realiza en grupos de **máximo 2 personas**. Pero no existe problema si usted desea hacerla de forma individual.
- La entrega es a través de u-cursos a más tardar el día estipulado en la misma plataforma. A las tareas atrasadas se les descontará un punto por día.
- El formato de entrega es este mismo **Rmarkdown** y un **html** con la tarea desarrollada. Por favor compruebe que todas las celdas han sido ejecutadas en el archivo html.
- Al momento de la revisión tu código será ejecutado. Por favor verifica que tu entrega no tenga errores de compilación.
- No serán revisadas tareas desarrolladas en Python.
- Está **PROHIBIDO** la copia o compartir las respuestas entre integrantes de diferentes grupos.
- Pueden realizar consultas de la tarea a través de U-cursos y/o del canal de Discord del curso. 


### **Referencias:**<a name="id3"></a>

Slides de las clases:

- [Introduction to Statistical Thinking](https://github.com/dccuchile/CC6104/blob/master/slides/1_1_ST-intro.pdf)
- [Introduction to R](https://github.com/dccuchile/CC6104/blob/master/slides/1_2_ST-R.pdf)
- [Descriptive Statistics](https://github.com/dccuchile/CC6104/blob/master/slides/1_3_ST-explore.pdf)
- [Probability](https://github.com/dccuchile/CC6104/blob/master/slides/1_4_ST-prob.pdf)

Videos de las clases:

- Introduction to Statistical Thinking: [video1](https://youtu.be/X4SqJu6lExM) [video2](https://youtu.be/YbiQU5TTBX4)
- Introduction to R: [video1](https://youtu.be/MbeLD3hWWVo) [video2](https://youtu.be/9W_eWCy86F4) [video3](https://youtu.be/QvFXSw2-1r4) [video4](https://youtu.be/y4JY7klrbfQ)
- Descriptive Statistics: [video1](https://youtu.be/kWNskZ8_98o) [video2](https://youtu.be/_FJ8x9M4b1w) [video3](https://youtu.be/m7VBNZ2mYWI) [video4](https://youtu.be/ylGMJ_aSQk0)
- Probability: [video1](https://youtu.be/R9AVYV73m1M) [video2](https://youtu.be/zubh1jbRiKE) [video3](https://youtu.be/uiwToagp0z4) [video4](https://youtu.be/RlhN3t_VIyw) [video5](https://youtu.be/4kV1dBaeWVc) [video6](https://youtu.be/MGyXc70JdSk)


# Primera Parte: Preguntas Teóricas<a name="id4"></a>
A continuación, se presentaran diferentes preguntas que abordan las temáticas vistas en clases. Por favor responda cada una de estas preguntas de forma breve, no más de 4 o 5 lineas.

#### **Pregunta 1:**
¿Por qué la estadística es importante?, ¿Que nos permite realizar con los datos?. De algún ejemplo.

> Porque la estadística es una herramienta que trabaja el análisis e interpretación de datos, permitiendonos Describir, Decidir y Predecir el mundo que nos rodea.
>
> Un ejemplo visto en clase es el estudio de las grasas saturadas. Una idea común es que comer muchas grasas saturadas, o incluso consumir un poco de ellas es muy perjudicial a la salud. Pero resulta que después de un estudio estadístico se pudo determinar que no solo no son malas, si no que hasta cierto punto estas son beneficiosas para la salud.
>
> De no ser por la estadística, no podríamos haber descrito realmente la influencia de las grasas saturadas sobre la salud, y nos hubieramos basado en un pensamiento más Eurístico que Empírico para la desicion de dietas, nutrición, etc.

#### **Pregunta 2:**
Un amigue cercano a usted le comenta que le preocupa salir a la calle cuando hay ofertas en los helados, esto debido a que ha visto el siguiente titular en un famoso diario chileno: "El aumento en la compra de helados tiene una alta correlación con la muerte de personas en Santiago". ¿Que le recomendaría a su amigue sobre el titular leído?, ¿Debería preocuparse tanto?.

> Le recomendaría que debería dejar de preocuparse, dado que el titular divulga probablemente información falsa y sin un estudio estadístico exhastivo. 
>
>En la disciplina de la estadística es muy dificil saber cuando tenemos causalidades directas entre variables, dado que varias relaciones ocultan otras realidades y relaciones ocultas, las cuales llamamos *variables de confusión*. 
>
>Lo anterior lleva a que la **Correlación entre variables no implica Causalidad**, lo cuál en palabras más simples y aplicado al ejemplo significa que *no porque aumente la cantidad de ofertas de helados significa que más gente va a morir*. No estamos considerando otras variables ocultas, como por ejemplo, probablemente todo lo anterior se debe al aumento de la temperatura en la ciudad de Santiago.

#### **Pregunta 3:**
Señale las diferentes aplicaciones que poseen las visualizaciones: Boxplot, histograma, gráfico de pie y scatterplot.

> **Boxplot**: Nos permite visualizar la **distribución de datos numéricos**, pero a diferencia de los gráficos de funciones de densidad o probabilidad como la normal, este segmenta los datos en cuartiles y es capaz de mostrarlos utilizando una única dimensión. La marca central indica la mediana, el rectángulo central indica donde se ubica el 25% de los datos menores y 25% de los datos mayores a la mediana, mientras que los *bigotes* indican los valores más externos. Este a su vez sirve para poder observar outliers o valores atípicos dentro de los datos.
>
> **Histograma**: Nos permiten visualizar la **distribución completa** de los datos numéricos, ya sean discretos o continuos (utilizando rangos de valores). Al visualizar por completo la distribución de los datos por rangos, podemos tener una idea de la forma que toma la función de de densidad de probabilidad del dataset.
>
> **Gráfico de Pie**: A diferencia de los gráficos anteriores, el Pie Chart sirve para **visualizar proporciones numéricas** de un conjunto de datos. Es útil cuando queremos visualizar distribuciones de variables categóricas, pero no es viable para mostrar demasiados datos al dificultar su comprensión.
>
> **Scatterplot**: Este gráfico utiliza coordenadas cartesianas para graficar datos numéricos con dos o más variables. Es útil para apreciar el nivel de **dependencia y correlación entre dos variables numéricas** de un dataset, sean lineales o no lineales.


#### **Pregunta 4:**
Suponga que esta estudiando la diferencia en los sueldos de las personas que viven en Santiago y Rancagua. Suponiendo que los datos poseen outliers, ¿Que métrica de resumen utilizaría para comparar los datos?. Justifique su respuesta.

> Utilizaría la **mediana**, ya que a diferencia del **promedio** esta es más robusta ante valores extremos en los datos, debido a que esta depende de la cantidad y densidad de los datos en vez de sus valores.

#### **Pregunta 5:**
En base al mismo dataset de sueldos para las regiones de Santiago y Rancagua, le comentan que existe un error en los datos y que estos deben ser modificados aumentando un 10% el valor original y sumando $15.000$ a cada uno de los datos. ¿Como se ve afectada la media, mediana y desviación estándar con esta modificación?. Explique a través de ecuaciones el cambio que experimentan las métricas de resumen respecto al valor original, considere para el caso de la media $\bar{X}_{old} = \dfrac{1}{m} \sum^{m}_{i=1} x_i$ y $sd_{old} = \sqrt{\dfrac{1}{(m-1)}\sum_{i=1}^{m}(x_i-\bar{x})^{2}}$ para la desviación estándar.

> **Promedio:**
>
> $\bar{X}_{new} = \dfrac{1}{m} \sum^{m}_{i=1} 1.1x_i + 15000 = \dfrac{1.1}{m} \sum^{m}_{i=1} x_i + \dfrac{1}{m} \sum^{m}_{i=1} 15000$
>
> Donde la sumatoria de los x de al final corresponde justo al promedio anterior, teniendo que:
>
> $\bar{X}_{new} = 1.1 \bar{X}_{old} + 15000$
>
> Que es justamente la modificación realizada a cada dato por separado. Luego este cambió linealmente respecto a la modificación.
>
> **Mediana:** Debido a que esta depende de la cantidad de datos y no de sus valores, esta variará de la misma forma en la que lo hace la mediana anterior. Luego la mediana también cambia linealmente respecto a la modificación.
>
>
> **Desviación Estándar:**
>
> $sd_{new} = \sqrt{\dfrac{1}{(m-1)}\sum_{i=1}^{m}((1.1x_i + 15000) - (1.1\bar{x} + 15000)^{2}} = \sqrt{\dfrac{1.1^{2}}{(m-1)}\sum_{i=1}^{m}(x_i - \bar{x})^{2}}$
> 
> Donde la sumatoria final corresponde justamente a la desviación estandar anterior pero por un factor 1.1, teniendo que:
>
> $sd_{new} = 1.1 sd_{old}$
>
> Que es el la pendiente de la modificiación realizada a los datos, Luego este aumentó en un 10 porciento.

#### **Pregunta 6:**
Suponga que debe responder un examen sorpresa de 10 preguntas, con 5 alternativas por cada pregunta. ¿Cual es la probabilidad de obtener mas de 5 alternativas correctas si responde de forma aleatoria todo el examen?.

**Nota:** Puede resolver el ejercicio desarrollándolo a mano o utilizando código en R. 

``` {r eval=TRUE}
# Debido a que queremos calcular la probabilidad de un suceso dicotómico, es claro
# que tenemos una distribución Binomial:

# La probabilidad de responder cada pregunta correctamente es de un quinto:
p = 0.2
# Sabemos que son más de 5 preguntas de 10: 
x = c(6:10)
size = 10

# Luego la probabilidad solicitada es:
prob = 0
for(val in x){
  prob = prob + dbinom(val, size, p)
}

prob
```

#### **Pregunta 7:**
Supongamos que el 10% de los alumnos del curso utilizan Macintosh, el 60% utiliza Windows y el 30% utiliza Linux. Supongamos que el 50% de los usuarios de Mac, el 78% de los usuarios de Windows y el 20% de los usuarios de Linux han sucumbido bajo un terrible virus. Al seleccionar una persona al azar nos enteramos de que su sistema está infectado por el virus. ¿Cuál es la probabilidad de que sea un alumno con Windows?.

> Recopilemos las probabilidades conocidas:
>
> **SO:** M 10%, W 60%, 30$ L.
>
> **Virus:** M 50%, 78% W, 20% L.
>
> Sabiendo que la persona escogida si tiene un virus, debemos calcular la probabilidad de que el alumno tenga windows, esto es, calcular $\mathbb{P}( \text{Windows} | \text{Virus})$. Ahora por Teorema de Bayes, tenemos que $\mathbb{P}(X | Y) = \frac{\mathbb{P}(Y | X)*\mathbb{P}(X) }{\mathbb{P}(Y)}$, y definiendo $X$ como el evento de que alguien tenga Windows, e $Y$ como el evento de que alguien tenga el Virus, tendremos lo siguiente:
>
> $\mathbb{P}(\text{Windows} | \text{Virus}) = \frac{\mathbb{P}(\text{Virus} | \text{Windows}) * \mathbb{P}(\text{Windows})}{\mathbb{P}(\text{Virus})}$
>
> $\mathbb{P}(\text{Windows} | \text{Virus}) = \frac{0.78 * 0.6}{\mathbb{P}(\text{Virus})}$
>
> Ahora, usando que:
>
> $\mathbb{P}(X) = \sum^{n}_{i=1} \mathbb{P}(X, Y_{i}) = \sum^{n}_{i=1} \mathbb{P}(X | Y_{i}) * \mathbb{P}(Y_{i})$
>
> Podemos calcular $\mathbb{P}(\text{Virus}) = \sum^{n}_{i=1} \mathbb{P}(X | Y_{i}) * \mathbb{P}(Y_{i}) = 0.5*0.1 + 0.78*0.6 + 0.2*0.3 = 0.578$ 
>
> Finalmente $\mathbb{P}(\text{Windows} | \text{Virus}) = \frac{0.78 * 0.6}{0.578} \approx 0.81$


#### **Pregunta 8:**
Señale si las siguientes declaraciones son verdaderas o falsas respecto a las variables aleatorias:

- [ ] Como las variables aleatorias son funciones que nos permiten obtener valores de probabilidad, siempre podemos obtener $\mathbb{P}(X=x)>0$ evaluando en una $f(x)$ continua y discreta.
- [ ] Una PDF bien definida solo puede tener valores menores a 1 y un área debajo de la curva igual a 1.
- [ ] La CDF puede ser representada como la integral de la PDF y PMF.
- [ ] Una CDF es definida para todo x, continua hacia la derecha y no es decreciente.

> Falso, dado que para una $f(x)$ continua $\mathbb{P}(X=x)$ siempre es 0, solo pueden evaluarse en rangos.
>
> Falso, ya que una PDF si puede tener valores iguales o mayores a 1.
>
> Falso, ya que en el caso de las PMF estamos con variables discretas, por lo que no es una función integrable. En vez de eso es necesaria una sumatoria.
>
> Verdadera.

#### **Pregunta 9:**
Una famosa fabrica de dulces señala que solo el $5\%$ de sus dulces contienen menos de $350$ gramos. Si los dulces elaborados por la fabrica distribuyen de forma normal, con media $\mu$ y desviación estándar $11.2$. Responda las siguientes preguntas:

- a) Encuentre la media del producto.
- b) Señale el porcentaje de dulces que se encuentran sobre los $390$ gramos.

**Nota:** Puede ser útil https://www.statskingdom.com/z_table.html

> Utilizando el enlace podemos determinar que $\mathbb{P}(X < -1.645) \approx 0.05$. Luego utilizando que $X = \mu + \sigma Z$ tenemos que:
>
> $350 = \mu - 1.645 \sigma$
>
> $\mu = 350 + 1.645*11.2 \approx 368.4$. Finalmente $\mu  \approx 368.4$ (a)
>
> Ahora debemos transformar de la distribución actual a la normal, lo cuál se hace utilizando $Z = \frac{X - \mu}{\sigma}$. Así desarrollamos como sigue:
>
> $Z = \frac{390 - 368.4}{11.2} \approx 1.93$. Y viendo la tabla del enlace podemos determinar que $\mathbb{P}(X < 1.93) \approx 0.97 \Rightarrow \mathbb{P}(X > 1.93) \approx 1 - 0.97 = 3 \%$ (b)

---

# Segunda Parte: Elaboración de Código<a name="id5"></a>

En la siguiente sección deberá resolver cada uno de los experimentos computacionales a través de la programación en R. Para esto se le aconseja que cree funciones en R, ya que le facilitará la ejecución de gran parte de lo solicitado.

### Pregunta 1: Visualización de Datos

Para esta pregunta usted deberá trabajar en base al conjunto de datos `hearth_database.csv`, el cual esta compuesto por las siguientes variables:

- target: Señala si el paciente tuvo un infarto.
- sex: Sexo de los sujetos de prueba.
- fbs: Azúcar en la sangre con ayunas. Esta variable señala solo si se encuentra <=120 o >120.
- exang: Angina de pecho inducida por el ejercicio.
- cp: Tipo de dolor de pecho.
- restecg: Resultados electrocardiográficos en reposo.
- slope: Pendiente del segmento ST máximo de ejercicio.
- ca: Número de buques principales.
- thal: Thalassemia.
- age: Edad en años.
- trestbps: Presión arterial en reposo.
- chol: colesterol sérico en mg/dl.
- thalach: Frecuencia cardíaca máxima alcanzada.
- oldpeak: Depresión del ST inducida por el ejercicio en relación con el reposo.

En base al dataset propuesto realice un análisis exploratorio de los datos (EDA). Para su análisis enfoquen el desarrollo en las siguientes tareas:

- [ ] Obtenga la media, mediana, quintiles y valores máximos desde los datos que componen el dataset.
- [ ] Obtenga la Matriz de correlación de Pearson y visualice la relación entre las variables numéricas.
- [ ] Visualice los boxplot para las variables numéricas.
- [ ] Visualice a través de un histograma como distribuyen las variables respecto a los TARGET.

**Respuesta**

```{r eval=TRUE}
hdb <- read.csv("hearth_database.csv")
head(hdb)
```

Es importante chequear si es que existen valores nulos dentro del dataset antes de realizar la exploración sobre este. Para revisar la presencia de valores inexistentes se utilizará la función is.na(), que retorna un valor de verdad dependiendo si el dato es NA o no. Realizaremos la operación suma sobre estos valores booleanos, si la suma resultante es 0 quiere decir que no se encontraron valores en el dataset en que is.na() retornará TRUE, lo que nos indicaría que no hay datos inexistentes en las columnas.

```{r eval=TRUE}
sum(is.na(hdb))
```
La función ```summary``` aplica estadísticas a cada columna. En particular, indica el promedio, mediana, quantiles, máximo, mínimo, entre otros. Además, cambiaremos el nombre de la columna age que tenía un error de tipado.

```{r eval=TRUE}

colnames(hdb)[10] <- "age"
summary(hdb)

```

Para conseguir los quintiles de cada uno de los datos numéricos, utilizaremos la función de R quantile (por algun motivo el for sobre 7 a 14 no retornaba los resultados, por lo que decidí ir uno por uno):
```{r eval=TRUE}
colnames(hdb)[7]
quantile(hdb[, 7], seq(0, 1, 0.2))

colnames(hdb)[8]
quantile(hdb[, 8], seq(0, 1, 0.2))

colnames(hdb)[9]
quantile(hdb[, 9], seq(0, 1, 0.2))

colnames(hdb)[10]
quantile(hdb[, 10], seq(0, 1, 0.2))

colnames(hdb)[11]
quantile(hdb[, 11], seq(0, 1, 0.2))

colnames(hdb)[12]
quantile(hdb[, 12], seq(0, 1, 0.2))

colnames(hdb)[13]
quantile(hdb[, 13], seq(0, 1, 0.2))

colnames(hdb)[14]
quantile(hdb[, 14], seq(0, 1, 0.2))

```

Ahora veremos la Matriz de Pearson para visualizar las correlaciones entre todas las variables numéricas:

```{r eval=TRUE}
library(ggplot2)

```



Boxplots

```{r eval=TRUE}
ggplot(data = hdb, aes(y = slope , x = target)) +
  geom_boxplot() 
ggplot(data = hdb, aes(y = ca , x = target)) +
  geom_boxplot() 
ggplot(data = hdb, aes(y = thal , x = target)) +
  geom_boxplot() 
ggplot(data = hdb, aes(y = age , x = target)) +
  geom_boxplot() 
ggplot(data = hdb, aes(y = trestbps , x = target)) +
  geom_boxplot() 
ggplot(data = hdb, aes(y = chol , x = target)) +
  geom_boxplot() 
ggplot(data = hdb, aes(y = thalach , x = target)) +
  geom_boxplot() 
ggplot(data = hdb, aes(y = oldpeak , x = target)) +
  geom_boxplot()

```





---

### Pregunta 2: Teorema Central del Limite

Pruebe el teorema central del limite aplicando un muestreo de la media en las distribuciones Poisson, Exponencial y una a su elección. Grafique los resultados obtenidos y señale aproximadamente el numero de muestreos necesarios para obtener el resultado esperado, pruebe esto con las siguientes cantidades de muestreo $\{10,100,1000,5000\}$. ¿El efecto ocurre con el mismo número de muestreo para todas las distribuciones?.

**Respuesta**
Primero creamos nuestros vectores con datos de distribución aleatorios

```{r eval=TRUE}
# Elementos aleatorios con distribución de Poisson
poisson_1 <- rpois(10, lambda = 3)
poisson_2 <- rpois(100, lambda = 3)
poisson_3 <- rpois(1000, lambda = 3)
poisson_4 <- rpois(5000, lambda = 3)
  
# Elementos aleatorios con distribución Exponencial
exp_1 <- rexp(10, rate = 1)
exp_2 <- rexp(100, rate = 1)
exp_3 <- rexp(1000, rate = 1)
exp_4 <- rexp(5000, rate = 1)

# Elementos aleatorios con distribución Normal
norm_1 <- rnorm(10, 0, 1)
norm_2 <- rnorm(100, 0, 1)
norm_3 <- rnorm(1000, 0, 1)
norm_4 <- rnorm(5000, 0, 1)

```

Ya teniendo todos nuestros valores aleatorios, pasaremos a graficarlos en histogramas con sus densidades respectivas con el fin de observar que tanto se asimilan en su mediana y en la función de densidad teórica de cada distribución, según lo esperado en el Teorema Central del Límite

#### Poisson
Partiendo por la distribución de Poisson, el factor lamda ocupado para la generación de valores aleatorios corresponde justamente a su media.

```{r eval=TRUE}
# Primero creamos un dataframe de los datos aleatorios, con atributo 'vals'
pois1 <- data.frame(vals=poisson_1)
# Posteriormente creamos el plot que mostraremos en pantalla
p1<-ggplot(pois1, aes(x=vals)) + # Elegimos el dataframe a plotear
  geom_histogram(aes(y=..density..), colour="black", fill="white") + # Creamos el Histograma
  geom_density(alpha=.2, fill="#FF6666") + # Ploteamos la densidad del dataframe
  geom_vline(aes(xintercept=mean(vals)),
            color="blue", linetype="solid", size=1) + # Ploteamos la mediana del dataframe
  geom_vline(aes(xintercept=3),
            color="red", linetype="solid", size=1) +  # Ploteamos además el lambda
  ggtitle("Poisson Distribution \n n = 10 , lambda = 3") +
  theme(plot.title = element_text(hjust = 0.5)) # Creamos el título y su tema
p1  # Finalmente ploteamos toda la figura descrita anteriormente

pois2 <- data.frame(vals=poisson_2)
p2<-ggplot(pois2, aes(x=vals)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +
  geom_vline(aes(xintercept=mean(vals)),
            color="blue", linetype="solid", size=1) +
  geom_vline(aes(xintercept=3),
            color="red", linetype="solid", size=1) +
  ggtitle("Poisson Distribution \n n = 100 , lambda = 3") +
  theme(plot.title = element_text(hjust = 0.5))
p2

pois3 <- data.frame(vals=poisson_3)
p3<-ggplot(pois3, aes(x=vals)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +
  geom_vline(aes(xintercept=mean(vals)),
            color="blue", linetype="solid", size=1) +
  geom_vline(aes(xintercept=3),
            color="red", linetype="solid", size=1) +
  ggtitle("Poisson Distribution \n n = 1000 , lambda = 3") +
  theme(plot.title = element_text(hjust = 0.5))
p3

pois4 <- data.frame(vals=poisson_4)
p4<-ggplot(pois4, aes(x=vals)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +
  geom_vline(aes(xintercept=mean(vals)),
            color="blue", linetype="solid", size=1) +
  geom_vline(aes(xintercept=3),
            color="red", linetype="solid", size=1) +
  ggtitle("Poisson Distribution \n n = 5000 , lambda = 3") +
  theme(plot.title = element_text(hjust = 0.5))
p4
```

Notemos primero que a medida que aumentaba la cantidad de datos, la media muestral fué acercandose cada vez más a la media teórica. Por otra parte, la forma oscilante de la densidad de los datos se deben a que estos son datos discretos y no continuos, pero sus cimas forman justamente una distribución de Poisson como tal.

Solo basta decir que este no toma muy claramente la distribución esperada por el Teorema Central del Límite hasta los 50000 datos, esto primero por las diferencias notables de la media teórica con la calculada hasta esta cantidad de datos, y por la forma de los datos con menor probabilidad.

#### Exponencial
Continuando con la distribución Exponencial, notemos que su media corresponde justamente al rate indicado en la generación de los datos aleatorios. Así, podemos generar los siguientes gráficos:
```{r eval=TRUE}
exp1 <- data.frame(vals=exp_1)
e1<-ggplot(exp1, aes(x=vals)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +
  geom_vline(aes(xintercept=mean(vals)),
            color="blue", linetype="solid", size=1) +
  geom_vline(aes(xintercept=1),
            color="red", linetype="solid", size=1) +
  ggtitle("Exponential Distribution \n n = 10 , rate = 1") +
  theme(plot.title = element_text(hjust = 0.5))
e1

exp2 <- data.frame(vals=exp_2)
e2<-ggplot(exp2, aes(x=vals)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +
  geom_vline(aes(xintercept=mean(vals)),
            color="blue", linetype="solid", size=1) +
  geom_vline(aes(xintercept=1),
            color="red", linetype="solid", size=1) +
  ggtitle("Exponential Distribution \n n = 100 , rate = 1") +
  theme(plot.title = element_text(hjust = 0.5))
e2

exp3 <- data.frame(vals=exp_3)
e3<-ggplot(exp3, aes(x=vals)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +
  geom_vline(aes(xintercept=mean(vals)),
            color="blue", linetype="solid", size=1) +
  geom_vline(aes(xintercept=1),
            color="red", linetype="solid", size=1) +
  ggtitle("Exponential Distribution \n n = 1000 , rate = 1") +
  theme(plot.title = element_text(hjust = 0.5))
e3

exp4 <- data.frame(vals=exp_4)
e4<-ggplot(exp4, aes(x=vals)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +
  geom_vline(aes(xintercept=mean(vals)),
            color="blue", linetype="solid", size=1) +
  geom_vline(aes(xintercept=1),
            color="red", linetype="solid", size=1) +
  ggtitle("Exponential Distribution \n n = 5000 , rate = 1") +
  theme(plot.title = element_text(hjust = 0.5))
e4

```

Es clara la similitud de la función de densidad con 5000 datos y la distribución exponencial teórica. En este caso la media calculada y la teórica en el último gráfico se sobreponen de forma perfecta, dejando en claro que ambas son prácticamente iguales.

Esta distribución, a diferencia de la de Poisson, ya obtiene similitudes con la distribución teórica a partir de los 1000 datos, por lo que podríamos decir que este necesita menor cantidad de datos para apreciar los resultados del Teorema Central del límite.


#### Normal
Finalmente, la distribución escogida para el final fué la bien conocida distribución Normal o Gaussiana, donde su media es 0 y su desviación 1:
```{r eval=TRUE}
norm1 <- data.frame(vals=norm_1)
n1<-ggplot(norm1, aes(x=vals)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +
  geom_vline(aes(xintercept=mean(vals)),
            color="blue", linetype="solid", size=1) +
  geom_vline(aes(xintercept=0),
            color="red", linetype="solid", size=1) +
  ggtitle("Normal Distribution \n n = 10 , mu = 0 , sigma = 1") +
  theme(plot.title = element_text(hjust = 0.5))
n1

norm2 <- data.frame(vals=norm_2)
n2<-ggplot(norm2, aes(x=vals)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +
  geom_vline(aes(xintercept=mean(vals)),
            color="blue", linetype="solid", size=1) +
  geom_vline(aes(xintercept=0),
            color="red", linetype="solid", size=1) +
  ggtitle("Normal Distribution \n n = 100 , mu = 0 , sigma = 1") +
  theme(plot.title = element_text(hjust = 0.5))
n2

norm3 <- data.frame(vals=norm_3)
n3<-ggplot(norm3, aes(x=vals)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +
  geom_vline(aes(xintercept=mean(vals)),
            color="blue", linetype="solid", size=1) +
  geom_vline(aes(xintercept=0),
            color="red", linetype="solid", size=1) +
  ggtitle("Normal Distribution \n n = 1000 , mu = 0 , sigma = 1") +
  theme(plot.title = element_text(hjust = 0.5))
n3

norm4 <- data.frame(vals=norm_4)
n4<-ggplot(norm4, aes(x=vals)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +
  geom_vline(aes(xintercept=mean(vals)),
            color="blue", linetype="solid", size=1) +
  geom_vline(aes(xintercept=0),
            color="red", linetype="solid", size=1) +
  ggtitle("Normal Distribution \n n = 5000 , mu = 0 , sigma = 1") +
  theme(plot.title = element_text(hjust = 0.5))
n4
```

Al igual que la distribución Exponencial, esta ya presenta similitudes claras a partir de los mil datos, en especial con el valor de la media muestral y teórica.

Ya teniendo todos los gráficos anteriores, podríamos determinar que no todas las distribuciones necesitan la misma cantidad de datos para que las implicancias del Teorema Central del límite se observen de forma clara. Un ejemplo es la distribución de Poisson, la cuál hasta los 1000 datos podríamos haber confundido con la distribución exponencial. 

---

### Pregunta 3: Ley de los Grandes Numeros.

#### Lanzamiento de monedas
Realice el experimento de lanzar una moneda cargada 1000 veces y observe el comportamiento que tiene la probabilidad de salir cara. Para realizar el experimento considere que la moneda tiene una probabilidad de $4/5$ de salir cara. Grafique el experimento para las secuencias de intentos que van desde 1 a 1000, señalando el valor en que converge la probabilidad de salir cara.

**Respuesta**

```{r eval=TRUE}

pcara = c(0)
caras = 0
for (lanzamientos in 1:1000) {
  coin <- sample(1:10, 1) # Tomaremos un numero al azar de 1 al 10
  if (coin < 9){
    caras <- caras + 1
  }
  prob = caras/lanzamientos
  pcara <- append(pcara, prob)
}

probs <- data.frame(probabilities = pcara)

# Gráfico de la convergencia
graph <- ggplot(probs, aes(x=c(0:1000), y=probabilities)) + # graficamos el dataframe
  geom_line() +    # graficamos los datos en forma de línea
  ylim(0, 1) +     # ponemos límites a los ejes
  xlim(0, 1000) +  
  geom_hline(aes(yintercept=0.8),  
            color="red", linetype="solid", size=1, alpha = 0.4) + # Ponemos una línea en 4/5
  xlab("Lanzamientos") +  
  ylab("Probabilidad") + 
  ggtitle("Probabilidad acumulada de salir cara") + 
  theme(plot.title = element_text(hjust = 0.5))
graph
```

#### El problema de Monty Hall 

Remontándonos en la televisión del año 1963, en USA existía un programa de concursos donde los participantes debían escoger entre 3 puertas para ganar un premio soñado. El problema del concurso era que solo detrás de 1 puerta estaba el premio mayor, mientras que detrás de las otras dos habían cabras como "premio". 

Una de las particularidades de este concurso, es que cuando el participante escogía una puerta, el animador del show abría una de las puertas que no fue escogida por el participante (Obviamente la puerta abierta por el animador no contenía el premio). Tras abrir la puerta, el animador consultaba al participante si su elección era definitiva, o si deseaba cambiar la puerta escogida por la otra puerta cerrada.

Imagine que usted es participante del concurso y desea calcular la probabilidad de ganar el gran premio **si cambia de puerta** en el momento que el animador se lo ofrece. Utilizando listas/arrays/vectores simule las puertas del concurso, dejando aleatoriamente el premio en alguna posición del array. Hecho esto, genere un numero de forma aleatoria para escoger una de las puerta (posiciones de la estructura), para luego ver si cambiando de posición tendrá mayores posibilidades de ganar el premio. Genere N veces el experimento y grafique cada una de las iteraciones, tal como se hizo en el ejercicio de las monedas.

<p align="center">
  <img src="https://brilliant-staff-media.s3-us-west-2.amazonaws.com/tiffany-wang/gWotbuEdYC.png" width="350">
</p>


**Respuesta:**

```{r eval=TRUE}
# Creamos una función que simule el juego
montyhall <- function(cambiar = TRUE){
  Puertas <- sample(1:3,3)             # Puertas donde la posición que tiene el 3 es el premio
  posicion <- sample(1:3,1)            # Elección del participante.
  
  if (cambiar){ 
    if (Puertas[posicion] == 3){
      Eleccion <- 0 
    } else {
      Eleccion <- 1
    }
  } else {
    if (Puertas[posicion] == 3){
      Eleccion <- 1
    } else {
      Eleccion <- 0
    }
  }
  
  return(Eleccion) # Retornamos la elección, esta puede que tenga el premio o no
}

# Función que simula N juegos
n_juegos <- function(n = 10 ,cambiar_puerta = TRUE){
  probs <- c(0)
  wins <- 0
  for (i in c(1:n)){
    eleccion <- montyhall(cambiar_puerta)
    if (eleccion == 1){
      wins <- wins + 1
    }
    prob = wins/i
    probs <- append(probs, prob)
  }
  return(probs) # Vector de largo n + 1 
}
```

Una primera impresión es que la probabilidad de éxito después de cambiar la primera puerta escogida sigue siendo la misma que no cambiarla, pero este razonamiento es erroneo. La probabilidad de éxito eligiendo solo una puerta es claramente de 1/3 y esta sigue manteniendose si en la segunda instancia se decide no cambiar la puerta, pero la probabilidad de éxito al cambiar de puerta en la segunda instancia es de 2/3, debido a que la probabilidad de que la primera puerta escogida sea el premio mayor es de 1/3 lo que implica que la probabilidad de que la segunda puerta tenga el premio mayor es de 1 - 1/3 = 2/3.

Con lo anterior, graficaremos la probabilidad acumulada por cada iteración del juego y además la constante 2/3 para observar cómo esta converge a este valor en vez de 1/3.

```{r eval=TRUE}
n <- 100
Probs <- data.frame(probabilities = n_juegos(n, TRUE)) # tiene n + 1 elementos
# Gráfico de la convergencia
graph <- ggplot(Probs, aes(x=c(0:n), y=probabilities)) + # graficamos el dataframe
  geom_line() +    # graficamos los datos en forma de línea
  ylim(0, 1) +     # ponemos límites a los ejes
  xlim(0, n) +  
  geom_hline(aes(yintercept=2/3),  
            color="red", linetype="solid", size=1, alpha = 0.4) +
  xlab("Número de Juego") +  
  ylab("Probabilidad") + 
  ggtitle("Probabilidad acumulada de ganar al cambiar puerta en Monty Hall") + 
  theme(plot.title = element_text(hjust = 0.5))
graph

```

---

### Pregunta 4: ¿Independencia?
Ustedes disponen de los dados D1 y D2, los cuales no están cargados y son utilizados para comprobar que $\mathbb{P}(AB)=\mathbb{P}(A)\mathbb{P}(B)$ cuando el evento A es independiente del B. Para estudiar la independencia considere que los eventos A y B se definen de la siguiente manera; sea A el evento dado por los valores obtenidos en el lanzamiento del dado D1, este está compuesto por $A=\{D1=1,D1=2,D1=6\}$. Por otro lado, el evento B viene dado por los valores obtenidos con el dado D2, el que está conformado por $B=\{D2=1,D2=2,D2=3,D2=4\}$. Con esto, tendremos un $\mathbb{P}(A)=1/2$, $\mathbb{P}(𝐵)=2/3$ y $\mathbb{P}(AB)=1/3$. Compruebe de forma gráfica que al realizar 1000 lanzamientos (u otro valor grande que usted desea probar) se visualiza que $\mathbb{P}(AB)=\mathbb{P}(A)\mathbb{P}(B)$. 

Hecho lo anterior, compruebe el comportamiento de un segundo grupo de eventos, dados por el lanzamiento de solo el dado D1. Donde, los eventos para D1 quedan definidos como: $A =\{D1=1,D1=2,D1=6\}$ y $B=\{D1=1,D1=2,D1=3\}$. ¿Se observa independencia en este experimento?.

Se le aconseja que para simular los lanzamientos de dados utilice la función `sample()` para generar valores aleatorios entre 1 y 6. Compruebe los números generados por la función con los casos favorables de cada uno de los eventos a ser estudiados.

**Respuesta:**

```{r eval=FALSE}
# Primer grupo de eventos
N_lan = 1000 # Numero de lanzamientos
  
L_A =  # Lanzamientos favorables A = c(1, 2, 6)
L_B =  # Lanzamientos favorables B = c(1, 2, 3, 4)
L_AB = # Lanzamientos favorables AB = c(1, 2)
```

```{r eval=FALSE}
# Segundo grupo de eventos
N_lan = 1000 # Numero de lanzamientos
  
L_A =  # Lanzamientos favorables A = c(1, 2, 6)
L_B =  # Lanzamientos favorables B = c(1, 2, 3)
L_AB = # Lanzamientos favorables AB = c(1, 2)
```

---

### Pregunta 5: La Ruina del Jugador
Un amigo ludópata suyo le comenta que el truco de jugar en el casino esta en no parar de apostar y apostando lo mínimo posible. Ya que así, tienes mas probabilidades de ganar el gran pozo que acumula el juego. Usted sabiendo la condición de su amigo, decide no creer en su conjetura y decide probar esto a través de un experimento.

Para realizar el experimento usted decide asumir las siguientes declaraciones, bajo sus observaciones:

- La probabilidad de ganar en un juego del casino es $9/19$
- Sabe que su amigo posee fondos en el rango de 0 a 200 dolares.
- Las apuestas como mínimo deben ser igual a 5 dolares.
- El monto de las apuestas no cambia y son siempre igual a la primera. Por ejemplo, si su amigo apuesta 50 dolares, todos los próximos juegos apuesta 50 hasta que se acaba su dinero.

En el experimento deberá obtener la evolución de los fondos hasta que el jugador se queda sin fondos para jugar. Puede ser útil seguir la lógica de una moneda cargada para realizar esto. Pruebe esto con una apuesta igual a 5, 25 y 50 graficando los resultados. Comente los resultados obtenidos.

**Respuesta**

```{r eval=TRUE}
# Función para obtener el desarrollo de las apuestas
ruina <- function(fondos = 100, apuesta = 5){
  vec_fondos <- c(fondos)
  while (0<fondos & fondos<200) {
    bet <- sample(1:19, 1) # Tomaremos un numero al azar de 1 al 19
    if (bet < 10){
      fondos <- fondos + apuesta
    } else{
      fondos <- fondos - apuesta
    }
    vec_fondos <- append(vec_fondos, fondos)
  }
  return(vec_fondos) # Devuelve un vector con el desarrollo de los fondos
}

plot(ruina(), type="l", col="blue", xlab="N° de juegos", ylab="Fondos", main="Evolución de los fondos (apuesta = 5)")
plot(ruina(apuesta = 25), type="l", col="blue", xlab="N° de juegos", ylab="Fondos", main="Evolución de los fondos (apuesta = 25)")
plot(ruina(apuesta = 50), type="l", col="blue", xlab="N° de juegos", ylab="Fondos", main="Evolución de los fondos (apuesta = 50)")
```

Con lo anterior podemos notar que claramente nuestra conjetura es correcta, y que nuestro compañero está equivocado. Realizar siempre la misma apuesta con un valor bajo da la ilusión de ganar más, debido a que esta puede extenderse por más tiempo que con apuestas de mayor valor, además de que se necesita mayor cantidad de aciertos para poder llegar a los límites del fondo.

Por otro lado, con las apuestas de mayor valor se reducen rápidamente la cantidad de apuestas realizadas, debido a que pueden sobrepasar los límites del fondo más rápidamente respecto a las apuestas de menor valor. Por lo anterior podemos inferir que realizar apuestas de valores más altos tienen mayor probabilidad de generar muchas más ganancias por secuencias de victorias consecutivas, pero equivalentemente puede llevar a la quiebra más facilmente.


&nbsp;
<hr />
<p style="text-align: center;">A work by <a href="https://github.com/dccuchile/CC6104">CC6104</a></p>

<!-- Add icon library -->
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css">

<!-- Add font awesome icons -->
<p style="text-align: center;">
    <a href="https://github.com/dccuchile/CC6104"><i class="fab fa-github" style='font-size:30px'></i></a>
</p>

&nbsp;